I"=0<h4 id="table-of-contents">Table of Contents</h4>

<ul>
  <li><a href="#SafePDP">Safe Pontryagin Differentiable Programming</a></li>
  <li><a href="#LFDC">Learning from Incremental Directional Corrections</a></li>
  <li><a href="#LFSD">Learning from Sparse Demonstrations</a></li>
  <li><a href="#PDP">Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</a></li>
</ul>

<p><a name="SafePDP"></a></p>

<p><br /> 
<br /></p>

<h4 id="safe-pontryagin-differentiable-programming"><a href="https://arxiv.org/abs/2105.14937" target="_blank">Safe Pontryagin Differentiable Programming</a></h4>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<p><img src="/collections/figures/SafePDP.png" alt="Kitten" align="left" title="SafePDP" width="250" hspace="20" /></p>

<p>We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic safe differentiable framework to solve a broad class of safety-critical learning and control tasks – problems that require the guarantee of both immediate and long-term constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of state and input constraints by incorporating them into the cost and loss through barrier functions. We prove the following fundamental features of Safe PDP: first, both the constrained solution and its gradient in backward pass can be approximated by solving a more efficient unconstrained counterpart; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy using a barrier parameter; and third, importantly, any intermediate results throughout the approximation and optimization are strictly respecting all constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safe learning and control tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging control systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/sC81qc2ip8U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            Safe PDP for safe policy optimization.
        </div>
    </div>
    <div class="col-sm mt-2 mt-md-0">
          <iframe width="360" height="202" src="https://www.youtube.com/embed/vZVxgo30mDs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          <div class="caption">
            Safe PDP for safe motion planning
        </div>
    </div>
</div>

<div class="row">
      <div class="col-sm mt-3 mt-md-0">
    </div>
      <div class="col-sm mt-3 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/OBiLYYlWi98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
          Safe PDP for learning MPCs (i.e., jointly learning dynamics,  constraints, and control cost) from demonstrations.
      </div>
    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>

<p>Safe-PDP Package (in Python): <a href="https://github.com/wanxinjin/Safe-PDP" target="_blank">https://github.com/wanxinjin/Safe-PDP</a><br />
Safe-PDP Paper: <a href="https://arxiv.org/abs/2105.14937" target="_blank">https://arxiv.org/abs/2105.14937</a><br /></p>

<p><br /></p>

<hr />

<p><a name="LFDC"></a></p>

<p><br /> 
<br /></p>

<h4 id="learning-from-incremental-directional-corrections"><a href="https://arxiv.org/abs/2011.15014" target="_blank">Learning from Incremental Directional Corrections</a></h4>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<p><img src="/collections/figures/lfdc.png" alt="Kitten" align="left" title="SafePDP" width="350" hspace="20" />
This paper proposes a technique which enables a robot to learn a control objective function incrementally from human user’s corrections. The human’s corrections can be as simple as directional corrections—corrections that indicate the direction of a control change without indicating its magnitude—applied at some time instances during the robot’s motion. We only assume that each of the human’s corrections, regardless of its magnitude, points in a direction that improves the robot’s current motion relative to an implicit objective function. The proposed method uses the direction of a correction to update the estimate of the objective function based on a cutting plane technique. We establish the theoretical results to show that this process of incremental correction and update guarantees convergence of the learned objective function to the implicit one. The method is validated by two human-robot games, where human players teach a 2-link robot arm and a 6-DoF quadrotor system for motion planning in environments with obstacles, and also on a real  quadrotor system in a user study.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/sXXwBNZ9oP4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            Quadrotor learns from human's directional corrections in Environment 1.
        </div>
    </div>
    <div class="col-sm mt-2 mt-md-0">
        <iframe width="360" height="202" src="https://www.youtube.com/embed/6fP6uIAWxgs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          <div class="caption">
            Quadrotor learns from human's directional corrections in Environment 2.
        </div>
    </div>
</div>

<p>Codes (in Python): <a href="https://github.com/wanxinjin/Learning-from-Directional-Corrections" target="_blank">https://github.com/wanxinjin/Learning-from-Directional-Corrections</a><br />
Paper: <a href="https://arxiv.org/abs/2011.15014" target="_blank">https://arxiv.org/abs/2011.15014</a><br /></p>

<p><br /></p>

<hr />

<p><a name="LFSD"></a></p>

<p><br /> 
<br /></p>

<h4 id="learning-from-sparse-demonstrations"><a href="https://arxiv.org/abs/2008.02159" target="_blank">Learning from Sparse Demonstrations</a></h4>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<p><img src="/collections/figures/lfd.png" alt="Kitten" align="left" title="SafePDP" width="250" hspace="20" /></p>

<p>This paper develops a Continuous Pontryagin Differentiable Programming method which enables a robot to learn its control objective function from sparse demonstrations. The demonstrations are given as a small number of sparse waypoints;  the waypoints are desired outputs of robot trajectory at certain time instances, sparsely located in a time horizon. The speed of demonstration may be different from the actual execution of robot. The Continuous PDP simultaneously searches for a control objective function and a time-warping function such that the robot reproduced trajectory has the minimal discrepancy loss from the sparse demonstrations. The search process is based on the projected gradient descent on the discrepancy loss, and the key proposed technique is the Differential Pontryagin’s Maximum Principle, which allows to efficiently compute the analytical gradient of a trajectory of a continuous-time optimal control system with respect to system parameters.  We have evaluated different aspects of the proposed Continuous PDP  on a simulated two-link robot arm. We apply the method to learn motion planning of a 6-DoF maneuvering quadrotor in environments with obstacles.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/yHBjVdPXmEw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            Robot arm learns from sparse demostrations.
        </div>
    </div>
    <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/42AVnALrr_M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          <div class="caption">
            Quadrotor learns from sparse demostrations.
        </div>
    </div>
</div>

<p>Codes (in Python): <a href="https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations" target="_blank">https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations</a><br />
Paper: <a href="https://arxiv.org/abs/2008.02159" target="_blank">https://arxiv.org/abs/2008.02159</a><br /></p>

<p><br /></p>

<hr />

<p><a name="PDP"></a></p>

<p><br /> 
<br /></p>

<h4 id="pontryagin-differentiable-programming-an-end-to-end-learning-and-control-framework"><a href="https://papers.nips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf" target="_blank">Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</a></h4>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<p><img src="/collections/figures/PDP.png" alt="Kitten" align="left" title="SafePDP" width="250" hspace="20" /></p>

<p>This paper develops a Pontryagin Differentiable Programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP distinguishes from existing methods by two novel techniques: first, we differentiate through Pontryagin’s Maximum Principle, and this allows to obtain the analytical derivative of a trajectory with respect to tunable parameters within an optimal control system, enabling end-to-end learning of dynamics, policies, or/and control objective functions; and second, we propose an auxiliary control system in the backward pass of the PDP framework, and the output of this auxiliary control system is the analytical derivative of the original system’s trajectory with respect to the parameters, which can be iteratively solved using standard control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identification, and control/planning. We demonstrate the capability of the PDP in each learning mode on different high-dimensional systems, including multi-link robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.</p>

<div class="row">
    <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/5Jsu772Sqcg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            PDP for optimal control
        </div>
    </div>

        <div class="col-sm mt-2 mt-md-0">
<iframe width="360" height="202" src="https://www.youtube.com/embed/awVNiCIJCfs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
        <div class="caption">
            PDP for inverse reinforcement learning.
        </div>
    </div>

</div>

<p>PDP Package (in Python): <a href="https://github.com/wanxinjin/Pontryagin-Differentiable-Programming" target="_blank">https://github.com/wanxinjin/Pontryagin-Differentiable-Programming</a><br />
PDP Paper: <a href="https://papers.nips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf" target="_blank">https://papers.nips.cc/paper/2020/hash/5a7b238ba0f6502e5d6be14424b20ded-Abstract.html</a><br /></p>

<p><br /></p>

<hr />

:ET