I"Ä<h2 id="project-posts">Project Posts</h2>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>

<h3 id="task-driven-hybrid-model-reduction-for-dexterous-manipulation"><a href="../td_hybridreduction"><b>Task-Driven Hybrid Model Reduction for Dexterous Manipulation</b></a></h3>
<p style="margin-bottom:0.7cm; margin-left: 1.5cm"> </p>

<center>
<p style="margin-bottom:0.5cm; margin-left: 0.0cm">
<img src="../blogs/TRO_HybridReduction/figures/turning_webpage2.gif" width="350" />
<img src="../blogs/TRO_HybridReduction/figures/moving_webpage2.gif" width="350" />
</p>
</center>

<!--  ###### Arxiv: TBD

###### Code (Python): [https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction](https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction)

######  Webpage: TBD 
 -->

<blockquote>
  <h6 id="arxiv-tbd">Arxiv: TBD</h6>
  <h6 id="code-python-httpsgithubcomwanxinjintask-driven-hybrid-reduction">Code (Python): <a href="https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction">https://github.com/wanxinjin/Task-Driven-Hybrid-Reduction</a></h6>
  <h6 id="webpage-tbd">Webpage: TBD</h6>
  <details> <summary><span style="font-family:Georgia; font-size:0.9em;">Abstract (click to check abstract)</span></summary>
In contact-rich tasks, like dexterous manipulation, the hybrid nature of
making and breaking contact creates challenges for model representation and
control. For example, choosing and sequencing contact locations for in-hand
manipulation, where there are thousands of potential hybrid modes, is not
generally tractable. In this paper, we are inspired by the observation that far
fewer modes are actually necessary to accomplish many tasks. Building on our
prior work learning hybrid models, represented as linear complementarity
systems, we find a reduced-order hybrid model requiring only a limited number
of task-relevant modes. This simplified representation, in combination with
model predictive control, enables real-time control yet is sufficient for
achieving high performance. We demonstrate the proposed method first on
synthetic hybrid systems, reducing the mode count by multiple orders of
magnitude while achieving task performance loss of less than 5%. We also apply
the proposed method to a three-fingered robotic hand manipulating a previously
unknown object. With no prior knowledge, we achieve state-of-the-art
closed-loop performance in less than five minutes of online learning.
</details>
</blockquote>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>

<h3 id="-learning-from-human-directional-corrections-"><b> Learning from Human Directional Corrections </b></h3>
<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<center>
<iframe width="700" height="400" src="https://www.youtube.com/embed/Mwlwt055Tgg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>

<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<h6 id="arxiv-httpsarxivorgabs201115014">Arxiv: <a href="https://arxiv.org/abs/2011.15014" target="_blank">https://arxiv.org/abs/2011.15014</a></h6>

<h6 id="code-python-httpsgithubcomwanxinjinlearning-from-directional-corrections">Code (Python): <a href="https://github.com/wanxinjin/Learning-from-Directional-Corrections" target="_blank">https://github.com/wanxinjin/Learning-from-Directional-Corrections</a></h6>

<details> <summary>Abstract (click to check abstract):</summary>
This paper proposes a novel approach that enables a robot to learn an objective function incrementally from human directional corrections. Existing methods learn from human magnitude corrections; since a human needs to carefully choose the magnitude of each correction, those methods can easily lead to over-corrections and learning inefficiency. The proposed method only requires human directional corrections -- corrections that only indicate the direction of an input change without indicating its magnitude. We only assume that each correction, regardless of its magnitude, points in a direction that improves the robot's current motion relative to an unknown objective function. The allowable corrections satisfying this assumption account for half of the input space, as opposed to the magnitude corrections which have to lie in a shrinking level set. For each directional correction, the proposed method updates the estimate of the objective function based on a cutting plane method, which has a geometric interpretation. We have established theoretical results to show the convergence of the learning process. The proposed method has been tested in numerical examples, a user study on two human-robot games, and a real-world quadrotor experiment. The results confirm the convergence of the proposed method and further show that the method is significantly more effective (higher success rate), efficient/effortless (less human corrections needed), and potentially more accessible (fewer early wasted trials) than the state-of-the-art robot learning frameworks.
</details>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>

:ET