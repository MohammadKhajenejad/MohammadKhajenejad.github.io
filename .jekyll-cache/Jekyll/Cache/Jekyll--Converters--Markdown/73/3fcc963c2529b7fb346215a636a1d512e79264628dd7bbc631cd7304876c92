I"~J<p>I am a postdoctoral researcher in the <a href="https://www.grasp.upenn.edu/" target="_blank">GRASP Laboratory</a> at <a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a>, working with <a href="https://dair.seas.upenn.edu/" target="_blank">Prof. Michael Posa</a>, focusing on  contact-rich manipulation.</p>

<p>I obtained my Ph.D. in the area of Autonomy and Control in the <a href="https://engineering.purdue.edu/AAE" target="_blank">School of Aeronautics and Astronautics</a>, <a href="https://www.purdue.edu/" target="_blank">Purdue University</a> in July 2021. Prior to Purdue,  I worked as a research assistant  at <a href="https://www.tum.de/en/" target="_blank">Technical University of Munich</a>, Germany. I  obtained my   Master and Bachelor degrees in Control Science and Engineering from <a href="http://en.hit.edu.cn/" target="_blank">Harbin Institute of Technology</a>, China.</p>

<p align="center">
<a href="mailto:wanxinjin@gmail.com" target="_blank">Email</a> / 
<a href="https://github.com/wanxinjin" target="_blank">Github</a> / 
<a href="https://twitter.com/jinwanxin" target="_blank">Twitter</a> / 
<a href="https://scholar.google.com/citations?user=SoEC4h4AAAAJ&amp;hl=en" target="_blank">Google Scholar</a> 
</p>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="research-interests">Research Interests</h3>
<p>My research lies at the intersection of control, machine learning, and optimization, with emphasis on addressing the fundamental and pressing challenges in autonomous systems and human-robot systems.</p>

<body>
  <div style="width: 100%;">
      <div style="width: 18%; height: 100px; float: left; background: transparent;"> 
         <strong>Control + Learning</strong>
      </div>
      <div style="margin-left: 18%; height: 100px; background: transparent;"> 
          <ul>
            <li>Differentiable control and learning,  Safe learning and control,</li>
            <li>(Inverse) optimal control, (Inverse) reinforcement learning, </li> 
            <li> Hybrid control system, Robust control, Adversarial learning</li>
        </ul>
      </div>
  </div>
</body>

<body>
  <div style="width: 100%;">
      <div style="width: 18%; height: 100px; float: left; background: transparent;"> 
         <strong>Robotics + Human</strong>
      </div>
      <div style="margin-left: 18%; height: 100px; background: transparent;"> 
          <ul>
            <li>Learning with human-on-the-loop, Contact-rich robot manipulation, </li>
            <li> Human-robot teaming, Learning from demonstrations, </li> 
            <li>Human motor control anlaysis, Task and motion planning </li>
        </ul>
      </div>
  </div>
</body>
<p>My goal is to integrate
the complementary benefits of these three disciplines to develop new theories, methods, and systems that provision efficiency, reliablity, and interactive intelligence for next-generation autonomous systems and human-robot systems.</p>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="selected-publications--submissions">Selected Publications &amp; Submissions</h3>

<p style="margin-bottom:500; margin-left: -1.0cm"> </p>

<p><img src="collections/figures/learn_lcs.gif" alt="Kitten" title="SafePDP" width="160" align="left" hspace="25" vspace="0" /></p>
<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>
<p><strong>Learning Linear Complementarity Systems</strong> <br />
<b>Wanxin Jin</b>, Alp Aydinoglu, Mathew Halm, and Michael Posa<br />
<em>Arixv</em>, 2021 <br />
<a href="https://arxiv.org/abs/2112.13284" target="_blank">[PDF]</a> / 
<a href="https://github.com/wanxinjin/Learning-LCS" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>

<details>
  <summary>Abstract </summary>
This paper investigates the learning, or system identification, of a class of piecewise-affine dynamical systems known as linear complementarity systems (LCSs). We propose a violation-based loss which enables efficient learning of the LCS parameterization, without prior knowledge of the hybrid mode boundaries, using gradient-based methods. The proposed violation-based loss incorporates both dynamics prediction loss and a novel complementarity - violation loss. We show several properties attained by this loss formulation, including its differentiability, the efficient computation of first- and second-order derivatives, and its relationship to the traditional prediction loss, which strictly enforces complementarity. We apply this violation-based loss formulation to learn LCSs with tens of thousands of (potentially stiff) hybrid modes. The results demonstrate a state-of-the-art ability to identify piecewise-affine dynamics, outperforming methods which must differentiate through non-smooth linear complementarity problems.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/safepdp_uav_1.gif" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="10" />
<strong>Safe Pontryagin Differentiable Programming</strong> <br />
<b>Wanxin Jin</b>, Shaoshuai Mou, and George J. Pappas<br />
<em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021<br />
<a href="https://arxiv.org/abs/2105.14937" target="_blank">[PDF]</a> / 
<a href="https://github.com/wanxinjin/Safe-PDP" target="_blank">[Code]</a> / 
<a href="videos#SafePDP" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>

<details>
  <summary>Abstract </summary>
We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic framework to solve a broad class of safety-critical learning and control tasks -- problems that require the guarantee of safety constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of system constraints on states and inputs by incorporating them into the cost or loss through barrier functions. We prove three fundamentals of the proposed Safe PDP: first, both the solution and its gradient in the backward pass can be approximated by solving their more efficient unconstrained counterparts; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy by a barrier parameter; and third, importantly, all intermediate results throughout the approximation and optimization strictly respect the constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safety-critical tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/lfc_v.gif" alt="Kitten" title="lfc" width="150" align="left" hspace="30" vspace="10" />
<strong>Learning from Human Directional Corrections</strong> <br />
<b>Wanxin Jin</b>, Todd D Murphey, and Shaoshuai Mou<br />
Submitted to <em>IEEE Transactions on Robotics (T-RO)</em>,    Under review <br />
<a href="https://arxiv.org/abs/2011.15014" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/Learning-from-Directional-Corrections" target="_blank">[Code]</a> /
<a href="videos#LFDC" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper proposes an approach which enables a robot to learn a control
objective function incrementally from human's directional corrections. Existing
methods learn from human's magnitude corrections and require a human to
carefully choose correction magnitudes, which otherwise can easily lead to
over-correction and learning inefficiency. The proposed method only requires
human's directional corrections --- corrections that only indicate the
direction of a control change without indicating its magnitude --- applied at
some time instances during the robot's motion. We only assume that each of
human's corrections, regardless of its magnitude, points in a direction that
improves the robot's current motion relative to an implicit control objective
function. Thus, human's valid corrections always account for half of the
correction space. The proposed method uses the direction of a correction to
update the estimate of the objective function based on a cutting plane
technique. We have established the theoretical results to show that this
process guarantees the convergence of the learned objective function to the
implicit one. The proposed approach has been examined by numerical examples, a
user study on two human-robot games, and a real-world quadrotor experiment. The
results confirm the convergence of the approach and show that the approach is
significantly more effective (higher success rate), efficient/effortless (less
human corrections needed), and accessible (fewer early wasted trials) than the
state-of-the-art robot interactive learning schemes.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/lfsd_v.gif" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="10" />
<strong>Learning from Sparse Demonstrations</strong> <br />
<b>Wanxin Jin</b>, Todd D Murphey, Dana Kulic, Neta Ezer, and Shaoshuai Mou<br />
Submitted to <em>IEEE Transactions on Robotics (T-RO)</em>,   Conditionally Accepted<br />
<a href="https://arxiv.org/abs/2008.02159" target="_blank">[PDF]</a>/
<a href="https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations" target="_blank">[Code]</a> /
<a href="https://youtu.be/BYAsqMxW5Z4" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper develops the Continuous Pontryagin Differentiable Programming
(Continuous PDP) method that enables a robot to learn a control utility
function from a few number of sparsely demonstrated keyframes. The keyframes
are few desired sequential outputs that a robot is wanted to follow at certain
time instances. The duration of the keyframes may be different from that of the
robot actual execution. The method jointly searches for a robot control utility
function and a time-warping function such that the robot motion sequentially
follows the given keyframes with minimal discrepancy loss. Continuous PDP
minimizes the discrepancy loss using projected gradient descent, by efficiently
solving the gradient of robot motion with respect to the unknown parameters.
The method is first evaluated on a simulated two-link robot arm, and then
applied to a 6-DoF maneuvering quadrotor to learn a utility function from
keyframes for its motion planning in un-modeled environments with obstacles.
The results show the efficiency of the method, its ability to handle time
misalignment between keyframes and robot execution, and the generalization of
the learned utility function into unseen motion conditions.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/pdp_rocket.gif" alt="Kitten" title="SafePDP" width="200" align="left" hspace="30" vspace="10" />
<strong>Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</strong> <br />
<b>Wanxin Jin</b>, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou<br />
<em>Advances in Neural Information Processing Systems (NeurIPS),</em>  2020 <br />
<a href="https://papers.nips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/Pontryagin-Differentiable-Programming" target="_blank">[Code]</a> /
<a href="videos#PDP" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper develops a Pontryagin Differentiable Programming (PDP) methodology,
which establishes a unified framework to solve a broad class of learning and control
tasks. The PDP distinguishes from existing methods by two novel techniques: first,
we differentiate through Pontryagin’s Maximum Principle, and this allows to obtain
the analytical derivative of a trajectory with respect to tunable parameters within an
optimal control system, enabling end-to-end learning of dynamics, policies, or/and
control objective functions; and second, we propose an auxiliary control system in
the backward pass of the PDP framework, and the output of this auxiliary control
system is the analytical derivative of the original system’s trajectory with respect
to the parameters, which can be iteratively solved using standard control tools. We
investigate three learning modes of the PDP: inverse reinforcement learning, system
identification, and control/planning. We demonstrate the capability of the PDP in
each learning mode on different high-dimensional systems, including multi-link
robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.
</details>

<p style="margin-bottom:1.5cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/ioc_incomplete_2.png" alt="Kitten" title="SafePDP" width="200" align="left" hspace="30" vspace="20" />
<strong>Inverse Optimal Control from Incomplete Trajectory Observations</strong> <br />
<b>Wanxin Jin</b>,  Dana Kulic, Shaoshuai Mou, and Sandra Hirche <br />
<em>International Journal of Robotics Research (IJRR),</em> 40:848–865,
2021 <br />
<a href="https://journals.sagepub.com/doi/full/10.1177/0278364921996384" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/IOC-from-Incomplete-Trajectory-Observations" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This article develops a methodology that enables learning an objective function of an optimal control system from incomplete trajectory observations. The objective function is assumed to be a weighted sum of features (or basis functions) with unknown weights, and the observed data is a segment of a trajectory of system states and inputs. The proposed technique introduces the concept of the recovery matrix to establish the relationship between any available segment of the trajectory and the weights of given candidate features. The rank of the recovery matrix indicates whether a subset of relevant features can be found among the candidate features and the corresponding weights can be learned from the segment data. The recovery matrix can be obtained iteratively and its rank non-decreasing property shows that additional observations may contribute to the objective learning. Based on the recovery matrix, a method for using incomplete trajectory observations to learn the weights of selected features is established, and an incremental inverse optimal control algorithm is developed by automatically finding the minimal required observation. The effectiveness of the proposed method is demonstrated on a linear quadratic regulator system and a simulated robot manipulator.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/ioc_multiphase.gif" alt="Kitten" title="SafePDP" width="120" align="left" hspace="45" vspace="0" />
<strong>Inverse Optimal Control for Multiphase cost functions</strong> <br />
<b>Wanxin Jin</b>, Dana Kulic, Jonathan  Lin, Shaoshuai Mou, and Sandra Hirche <br />
<em>IEEE Transactions on Robotics (T-RO)</em>, 35(6):1387–1398,
2019 <br />
<a href="https://ieeexplore.ieee.org/document/8778698" target="_blank">[PDF]</a> / 
<a href="https://github.com/adaptivesystemslab/ioc" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
In this paper, we consider a dynamical system whose trajectory is a result of minimizing a multiphase cost function. The multiphase cost function is assumed to be a weighted sum of specified features (or basis functions) with phase-dependent weights that switch at some unknown phase transition points. A new inverse optimal control approach for recovering the cost weights of each phase and estimating the phase transition points is proposed. The key idea is to use a length-adapted window moving along the observed trajectory, where the window length is determined by finding the minimal observation length that suffices for a successful cost weight recovery. The effectiveness of the proposed method is first evaluated on a simulated robot arm, and then, demonstrated on a dataset of human participants performing a series of squatting tasks. The results demonstrate that the proposed method reliably retrieves the cost function of each phase and segments each phase of motion from the trajectory with a segmentation accuracy above 90%.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/DIOC.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="30" />
<strong>Distributed Inverse Optimal Control</strong> <br />
<b>Wanxin Jin</b> and Shaoshuai Mou <br />
<em>Automatica</em>, Volume 129, 2021 <br />
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0005109821001783" target="_blank">[PDF]</a> /
<a href="https://github.com/ZihaoLiang/Inverse-Optimal-Control-from-Demonstration-Segments" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper develops a distributed approach for inverse optimal control (IOC) in multi-agent systems. Here each agent can only communicate with certain nearby neighbors and only accesses segments of system’s trajectory, which is not sufficient for the agent to solve the IOC problem alone. By introducing the concept of the data effectiveness and bridging the connection between each segment and its contribution to solving IOC, we formulate the IOC problem as a problem of achieving least-square solutions via a distributed algorithm. Simulations are provided to validate the proposed distributed IOC approach.
</details>

<p style="margin-bottom:1.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="academic-honors--awards">Academic Honors &amp; Awards</h3>

<p style="margin-bottom:100; margin-left: -1.0cm"> </p>

<ul>
  <li>Best Student Paper Finalist at IEEE 40th Digital Avionics Systems Conference (DASC) — 09.2021</li>
  <li>ICON Outstanding Research Awards, Purdue University — 04.2021</li>
  <li>Magoon Award for Excellence in Teaching, Purdue University — 09.2020</li>
  <li>Ross Fellowship, Purdue University — 2017-2018</li>
  <li>First prize winner of Provincial Science and Technology Award, Heilongjiang, China – 06.2017</li>
</ul>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />
<p><br /></p>

<div style="text-align: right"> <a href="#top">Back to top</a> </div>

:ET