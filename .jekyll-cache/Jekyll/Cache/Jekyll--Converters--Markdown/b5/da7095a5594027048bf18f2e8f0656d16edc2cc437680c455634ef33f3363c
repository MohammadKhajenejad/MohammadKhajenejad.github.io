I">C<p>I am a postdoctoral researcher in the <a href="https://www.grasp.upenn.edu/" target="_blank">GRASP Laboratory</a> at <a href="https://www.upenn.edu/" target="_blank">University of Pennsylvania</a>, working with <a href="https://dair.seas.upenn.edu/michael-posa/" target="_blank">Prof. Michael Posa</a>.</p>

<p>I obtained my Ph.D. in the area of Autonomy and Control in the <a href="https://engineering.purdue.edu/AAE" target="_blank">School of Aeronautics and Astronautics</a>, <a href="https://www.purdue.edu/" target="_blank">Purdue University</a> in July 2021. Prior to Purdue,  I worked as a research assistant  at <a href="https://www.tum.de/en/" target="_blank">Technical University of Munich</a>, Germany. I  obtained my   Master and Bachelor degrees in Control Science and Engineering from <a href="http://en.hit.edu.cn/" target="_blank">Harbin Institute of Technology</a>, China.</p>

<p align="center">
<a href="mailto:wanxinjin@gmail.com" target="_blank">Email</a> / 
<a href="https://twitter.com/jinwanxin" target="_blank">Twitter</a> / 
<a href="https://github.com/wanxinjin" target="_blank">Github</a> / 
<a href="https://scholar.google.com/citations?user=SoEC4h4AAAAJ&amp;hl=en" target="_blank">Google Scholar</a> 
</p>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="research-interests">Research Interests</h3>
<p>My research lies at the intersection of   control, machine learning, and optimization, with  motivations stemming from  fundamental and pressing challenges in   robot and human-robot autonomy.</p>

<body>
  <div style="width: 100%;">
      <div style="width: 18%; height: 100px; float: left; background: transparent;"> 
         <strong>Control + Learning</strong>
      </div>
      <div style="margin-left: 18%; height: 100px; background: transparent;"> 
          <ul>
            <li>Differentiable control  and learning,  Safe learning and control</li>
            <li>(Inverse) optimal control, (Inverse) reinforcement learning </li> 
            <li>Differential games, Robust control, Adversarial learning</li>
        </ul>
      </div>
  </div>
</body>

<body>
  <div style="width: 100%;">
      <div style="width: 18%; height: 100px; float: left; background: transparent;"> 
         <strong>Robotics + Human</strong>
      </div>
      <div style="margin-left: 18%; height: 100px; background: transparent;"> 
          <ul>
            <li>Robot learning with human-on-the-loop, Human-robot collaboration</li>
            <li>Learning from demonstrations, Contact-rich robot manipulations</li> 
            <li>Motion and task planning, Computation of cognition &amp; motor control</li>
        </ul>
      </div>
  </div>
</body>
<p>My long-term goal  is to bring together the complementary benefits of the   three areas to develop new theories, methods, and systems that provision  efficiency,  safety, robustness,  long-duration adaptability  and can be effortlessly deployed onto   real-world robots and human-robot systems.</p>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="selected-publications--submissions">Selected Publications &amp; Submissions</h3>

<p style="margin-bottom:100; margin-left: -1.0cm"> </p>

<p><img src="collections/figures/SafePDP.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="10" />
<strong>Safe Pontryagin Differentiable Programming</strong> <br />
<b>Wanxin Jin</b>, Shaoshuai Mou, and George J. Pappas<br />
<em>Advances in Neural Information Processing Systems (NeurIPS), 2021,</em>  Accepted <br />
<a href="https://arxiv.org/abs/2105.14937" target="_blank">[PDF]</a> / 
<a href="https://github.com/wanxinjin/Safe-PDP" target="_blank">[Code]</a> / 
<a href="videos#SafePDP" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>

<details>
  <summary>Abstract </summary>
We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic safe differentiable framework to solve a broad class of safety-critical learning and control tasks -- problems that require the guarantee of both immediate and long-term constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of state and input constraints by incorporating them into the cost and loss through barrier functions. We prove the following fundamental features of Safe PDP: first, both the constrained solution and its gradient in backward pass can be approximated by solving a more efficient unconstrained counterpart; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy using a barrier parameter; and third, importantly, any intermediate results throughout the approximation and optimization are strictly respecting all constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safe learning and control tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging control systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/lfc_v.gif" alt="Kitten" title="lfc" width="150" align="left" hspace="30" vspace="10" />
<strong>Learning from Incremental Directional Corrections</strong> <br />
<b>Wanxin Jin</b>, Todd D Murphey, and Shaoshuai Mou<br />
Submitted to <em>IEEE Transactions on Robotics (T-RO), 2021,</em>    Under review <br />
<a href="https://arxiv.org/abs/2011.15014" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/Learning-from-Directional-Corrections" target="_blank">[Code]</a> /
<a href="videos#LFDC" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper proposes a technique which enables a robot to learn a control objective function incrementally from human user's corrections. The human's corrections can be as simple as directional corrections -- corrections that indicate the direction of a control change without indicating its magnitude -- applied at some time instances during the robot's motion. We only assume that each of the human's corrections, regardless of its magnitude, points in a direction that improves the robot's current motion relative to an implicit objective function. The proposed method uses the direction of a correction to update the estimate of the objective function based on a cutting plane technique. We establish the theoretical results to show that this process of incremental correction and update guarantees convergence of the learned objective function to the implicit one. The method is validated by two human-robot games, where human players teach a 2-link robot arm and a 6-DoF quadrotor system for motion planning in environments with obstacles, and also on a real  quadrotor system in a user study.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/lfd.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="10" />
<strong>Learning from Sparse Demonstrations</strong> <br />
<b>Wanxin Jin</b>, Todd D Murphey, Dana Kulic, Neta Ezer, and Shaoshuai Mou<br />
Submitted to <em>IEEE Transactions on Robotics (T-RO), 2021,</em>   Under review <br />
<a href="https://arxiv.org/abs/2008.02159" target="_blank">[PDF]</a>/
<a href="https://github.com/wanxinjin/Learning-from-Sparse-Demonstrations" target="_blank">[Code]</a> /
<a href="videos#LFSD" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper proposes an approach which enables a robot to learn an objective function from sparse demonstrations of an expert. The demonstrations are given by a small number of sparse waypoints; the waypoints are desired outputs of the robot's trajectory at certain time instances, sparsely located within a demonstration time horizon. The duration of the expert's demonstration may be different from the actual duration of the robot's execution. The proposed method enables to jointly learn an objective function and a time-warping function such that the robot's reproduced trajectory has minimal distance to the sparse demonstration waypoints. Unlike existing inverse reinforcement learning techniques, the proposed approach uses the differential Pontryagin's maximum principle, which allows direct minimization of the distance between the robot's trajectory and the sparse demonstration waypoints and enables simultaneous learning of an objective function and a time-warping function. We demonstrate the effectiveness of the proposed approach in various simulated scenarios. We apply the method to learn motion planning/control of a 6-DoF maneuvering unmanned aerial vehicle (UAV) and a robot arm in environments with obstacles. The results show that a robot is able to learn a valid objective function to avoid obstacles with few demonstrated waypoints.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/PDP.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="20" />
<strong>Pontryagin Differentiable Programming: An End-to-End Learning and Control Framework</strong> <br />
<b>Wanxin Jin</b>, Zhaoran Wang, Zhuoran Yang, and Shaoshuai Mou<br />
<em>Advances in Neural Information Processing Systems (NeurIPS),  2020</em> <br />
<a href="https://papers.nips.cc/paper/2020/file/5a7b238ba0f6502e5d6be14424b20ded-Paper.pdf" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/Pontryagin-Differentiable-Programming" target="_blank">[Code]</a> /
<a href="videos#PDP" target="_blank">[Videos]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper develops a Pontryagin Differentiable Programming (PDP) methodology,
which establishes a unified framework to solve a broad class of learning and control
tasks. The PDP distinguishes from existing methods by two novel techniques: first,
we differentiate through Pontryagin’s Maximum Principle, and this allows to obtain
the analytical derivative of a trajectory with respect to tunable parameters within an
optimal control system, enabling end-to-end learning of dynamics, policies, or/and
control objective functions; and second, we propose an auxiliary control system in
the backward pass of the PDP framework, and the output of this auxiliary control
system is the analytical derivative of the original system’s trajectory with respect
to the parameters, which can be iteratively solved using standard control tools. We
investigate three learning modes of the PDP: inverse reinforcement learning, system
identification, and control/planning. We demonstrate the capability of the PDP in
each learning mode on different high-dimensional systems, including multi-link
robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/ioc_incomplete.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="30" />
<strong>Inverse Optimal Control from Incomplete Trajectory Observations</strong> <br />
<b>Wanxin Jin</b>,  Dana Kulic, Shaoshuai Mou, and Sandra Hirche <br />
<em>The International Journal of Robotics Research (IJRR), 40(6-7):848–865,
2021</em> <br />
<a href="https://journals.sagepub.com/doi/full/10.1177/0278364921996384" target="_blank">[PDF]</a> /
<a href="https://github.com/wanxinjin/IOC-from-Incomplete-Trajectory-Observations" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This article develops a methodology that enables learning an objective function of an optimal control system from incomplete trajectory observations. The objective function is assumed to be a weighted sum of features (or basis functions) with unknown weights, and the observed data is a segment of a trajectory of system states and inputs. The proposed technique introduces the concept of the recovery matrix to establish the relationship between any available segment of the trajectory and the weights of given candidate features. The rank of the recovery matrix indicates whether a subset of relevant features can be found among the candidate features and the corresponding weights can be learned from the segment data. The recovery matrix can be obtained iteratively and its rank non-decreasing property shows that additional observations may contribute to the objective learning. Based on the recovery matrix, a method for using incomplete trajectory observations to learn the weights of selected features is established, and an incremental inverse optimal control algorithm is developed by automatically finding the minimal required observation. The effectiveness of the proposed method is demonstrated on a linear quadratic regulator system and a simulated robot manipulator.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/DIOC.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="30" />
<strong>Distributed Inverse Optimal Control</strong> <br />
<b>Wanxin Jin</b> and Shaoshuai Mou <br />
<em>Automatica, Volume 129, 2021</em> <br />
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0005109821001783" target="_blank">[PDF]</a> /
<a href="https://github.com/ZihaoLiang/Inverse-Optimal-Control-from-Demonstration-Segments" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
This paper develops a distributed approach for inverse optimal control (IOC) in multi-agent systems. Here each agent can only communicate with certain nearby neighbors and only accesses segments of system’s trajectory, which is not sufficient for the agent to solve the IOC problem alone. By introducing the concept of the data effectiveness and bridging the connection between each segment and its contribution to solving IOC, we formulate the IOC problem as a problem of achieving least-square solutions via a distributed algorithm. Simulations are provided to validate the proposed distributed IOC approach.
</details>

<p style="margin-bottom:1.0cm; margin-left: 0.5cm"> </p>

<p><img src="collections/figures/ioc_multiphase.png" alt="Kitten" title="SafePDP" width="150" align="left" hspace="30" vspace="10" />
<strong>Inverse Optimal Control for Multiphase cost functions</strong> <br />
<b>Wanxin Jin</b>, Dana Kulic, Jonathan  Lin, Shaoshuai Mou, and Sandra Hirche <br />
<em>IEEE Transactions on Robotics (T-RO), 35(6):1387–1398,
2019</em> <br />
<a href="https://ieeexplore.ieee.org/document/8778698" target="_blank">[PDF]</a> / 
<a href="https://github.com/adaptivesystemslab/ioc" target="_blank">[Code]</a></p>

<p style="margin-bottom:-0.4cm; margin-left: 0.5cm"> </p>
<details>
  <summary>Abstract </summary>
In this paper, we consider a dynamical system whose trajectory is a result of minimizing a multiphase cost function. The multiphase cost function is assumed to be a weighted sum of specified features (or basis functions) with phase-dependent weights that switch at some unknown phase transition points. A new inverse optimal control approach for recovering the cost weights of each phase and estimating the phase transition points is proposed. The key idea is to use a length-adapted window moving along the observed trajectory, where the window length is determined by finding the minimal observation length that suffices for a successful cost weight recovery. The effectiveness of the proposed method is first evaluated on a simulated robot arm, and then, demonstrated on a dataset of human participants performing a series of squatting tasks. The results demonstrate that the proposed method reliably retrieves the cost function of each phase and segments each phase of motion from the trajectory with a segmentation accuracy above 90%.
</details>

<p style="margin-bottom:1.8cm; margin-left: 0.5cm"> </p>

<hr />

<h3 id="academic-honors--awards">Academic Honors &amp; Awards</h3>

<p style="margin-bottom:100; margin-left: -1.0cm"> </p>

<ul>
  <li>Best Student Paper Finalist at IEEE 40th Digital Avionics Systems Conference (DASC) — 09.2021</li>
  <li>ICON Outstanding Research Awards, Purdue University — 04.2021</li>
  <li>Magoon Award for Excellence in Teaching, Purdue University — 09.2020</li>
  <li>Ross Fellowship, Purdue University — 2017-2018</li>
  <li>First prize winner of Provincial Science and Technology Award, Heilongjiang, China – 06.2017</li>
</ul>

<p style="margin-bottom:0.8cm; margin-left: 0.5cm"> </p>

<hr />
<p><br /></p>

<p><img src="https://profile-counter.glitch.me/{YOUR USER}/count.svg" alt="Visitor Count" /></p>
<div style="text-align: right"> <a href="#top">Back to top</a> </div>

:ET