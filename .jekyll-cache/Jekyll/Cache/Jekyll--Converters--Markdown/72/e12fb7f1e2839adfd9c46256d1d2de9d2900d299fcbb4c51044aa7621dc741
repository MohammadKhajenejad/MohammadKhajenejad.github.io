I"/<h2 id="project-posts">Project Posts</h2>

<hr />

<h3 id="task-driven-hybrid-model-reduction-for-dexterous-manipulation"><a href="../td_hybridreduction">Task-Driven Hybrid Model Reduction for Dexterous Manipulation</a></h3>
<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<!-- <img src="../collections/research/manipulation/three_finger_manipulation.gif"  width="400"  align="left" hspace="20" vspace=10 /> -->
<!-- In contact-rich tasks, like dexterous manipulation, the hybrid nature of making and breaking contact creates challenges for model representation and control. For example, choosing and sequencing contact locations for in-hand manipulation, where there are thousands of potential hybrid modes, is not generally tractable. In this paper, we are inspired by the observation that far fewer modes are actually necessary to accomplish many tasks. Building on our prior work learning hybrid models, represented as linear complementarity systems,
we find a reduced-order hybrid model requiring only a limited number of task-relevant modes. This simplified representation, in combination with model predictive control, enables real-time control yet is sufficient for achieving high performance. We demonstrate the proposed method first on synthetic hybrid systems, reducing the mode count by multiple orders of magnitude while achieving task performance loss of less than 5%. We also apply the proposed method to a three-fingered robotic hand manipulating a previously unknown object. With no prior knowledge, we achieve state-of-the-art closed-loop performance in less than five minutes of online learning.
 -->

<p style="margin-bottom:1.5cm; margin-left: 1.5cm">
<img src="../blogs/TRO_HybridReduction/figures/turning_webpage2.gif" width="350" />
<img src="../blogs/TRO_HybridReduction/figures/moving_webpage2.gif" width="350" />
</p>

<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>
<hr />
<p style="margin-bottom:1.0cm; margin-left: 1.5cm"> </p>

<h3 id="learning-from-human-directional-corrections"><a href="../td_hybridreduction">Learning from Human Directional Corrections</a></h3>
<p style="margin-bottom:0.5cm; margin-left: 1.5cm"> </p>

<p><img src="../collections/research/manipulation/three_finger_manipulation.gif" width="400" align="left" hspace="20" vspace="10" />
This paper proposes a novel approach that enables a robot to learn an objective function incrementally from human directional corrections. Existing methods learn from human magnitude corrections; since a human needs to carefully choose the magnitude of each correction, those methods can easily lead to over-corrections and learning inefficiency. The proposed method only requires human directional corrections â€“ corrections that only indicate the direction of an input change without indicating its magnitude. We only assume that each correction, regardless of its magnitude, points in a direction that improves the robotâ€™s current motion relative to an unknown objective function. The allowable corrections satisfying this assumption account for half of the input space, as opposed to the magnitude corrections which have to lie in a shrinking level set. For each directional correction, the proposed method updates the estimate of the objective function based on a cutting plane method, which has a geometric interpretation. We have established theoretical results to show the convergence of the learning process. The proposed method has been tested in numerical examples, a user study on two human-robot games, and a real-world quadrotor experiment. The results confirm the convergence of the proposed method and further show that the method is significantly more effective (higher success rate), efficient/effortless (less human corrections needed), and potentially more accessible (fewer early wasted trials) than the state-of-the-art robot learning frameworks.
â€”</p>
:ET